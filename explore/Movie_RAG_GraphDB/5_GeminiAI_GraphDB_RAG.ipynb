{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "load_dotenv()\n",
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the environment variables from your `.env` file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the LLM (e.g: GEMINI 2.5 Flash)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "client = ChatGoogleGenerativeAI(\n",
    "    model = os.getenv(\"MODEL_NAME\"),\n",
    "    temperature = os.getenv(\"TEMPERATURE\"),\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    ")\n",
    "\n",
    "nlp = spacy.load(os.getenv(\"EMBEDDING_MODEL_NAME\"))\n",
    "\n",
    "def embed_text(text:str)->List:\n",
    "    \"\"\"\n",
    "    Embeds the given text using the specified model.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to be embedded.\n",
    "\n",
    "    Returns:\n",
    "        List: A list containing the embedding of the text.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return doc.vector # 300-dim vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add Neo4j credentials (These information need to be kept secret)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"12345678\"\n",
    "NEO4J_DATABASE = 'neo4j'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217497/3535025753.py:1: LangChainDeprecationWarning: The class `Neo4jGraph` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-neo4j package and should be used instead. To use it run `pip install -U :class:`~langchain-neo4j` and import as `from :class:`~langchain_neo4j import Neo4jGraph``.\n",
      "  graph = Neo4jGraph(url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE)\n"
     ]
    }
   ],
   "source": [
    "graph = Neo4jGraph(url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample question for RAG:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What movies are about love?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get the questions embedding:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.67266995,  0.08774167, -0.54867333, -0.36176834, -0.04267567,\n",
       "       -0.02948567,  0.11805334, -0.40875998,  0.13430484,  1.8334517 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_embedding = embed_text(question)\n",
    "question_embedding[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform Similarity Search using the question's embedding on the vector index of the graph database and get the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'movie.title': 'Waiting to Exhale',\n",
       "  'movie.tagline': 'Friends are the people who let you be yourself... and never let you forget it.',\n",
       "  'score': 0.9490612745285034},\n",
       " {'movie.title': 'Grumpier Old Men',\n",
       "  'movie.tagline': 'Still Yelling. Still Fighting. Still Ready for Love.',\n",
       "  'score': 0.9432224035263062},\n",
       " {'movie.title': 'GoldenEye',\n",
       "  'movie.tagline': 'No limits. No fears. No substitutes.',\n",
       "  'score': 0.9357520937919617}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = graph.query(\"\"\"\n",
    "    with $question_embedding as question_embedding      // Use the provided question embedding as 'question_embedding'\n",
    "    CALL db.index.vector.queryNodes(                    // Call the vector index query function\n",
    "        'movie_tagline_embeddings',                     // Name of the vector index to query against\n",
    "        $top_k,                                         // Number of top results to retrieve\n",
    "        question_embedding                              // The question embedding to compare against\n",
    "        ) YIELD node AS movie, score                    // Yield each matched node and its similarity score\n",
    "    RETURN movie.title, movie.tagline, score            // Return the title, tagline, and similarity score of each movie\n",
    "    \"\"\",\n",
    "    params={\n",
    "        \"question_embedding\": question_embedding,       # Pass the question embedding as a parameter\n",
    "        \"top_k\": 3                                      # Specify the number of top results to retrieve\n",
    "    })\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pass the results to an LLM for the final answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the search results, here are the movies that appear to be about love:\n",
      "\n",
      "*   **Grumpier Old Men** (Tagline: \"Still Yelling. Still Fighting. Still Ready for Love.\")\n",
      "*   **Waiting to Exhale** (While the tagline focuses on friendship, the movie itself explores the romantic relationships and lives of its main characters.)\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"# Question:\\n{question}\\n\\n# Graph DB search results:\\n{result}\"\n",
    "\n",
    "system_msg = SystemMessage(\n",
    "            content=\"You will be given the user question along with the search result of that question over a Neo4j graph database. Give the user the proper answer.\"\n",
    "        )\n",
    "user_msg = HumanMessage(content=prompt)\n",
    "response = client.invoke([system_msg, user_msg])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "\n",
    "**Note: In this usecase, there is a higher chance of hallucination due to lack of enough evidence for the LLM to use its own judgment. The contents of the vector DB and the system role can address this issue to some extent.**\n",
    "\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second example (in one go):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the search results, the movies that are about adventure are:\n",
      "\n",
      "*   **GoldenEye**\n",
      "*   **Balto**\n",
      "*   **Tom and Huck**\n"
     ]
    }
   ],
   "source": [
    "question = \"What movies are about adventure?\"\n",
    "question_embedding = embed_text(question)\n",
    "result = graph.query(\"\"\"\n",
    "    with $question_embedding as question_embedding\n",
    "    CALL db.index.vector.queryNodes(\n",
    "        'movie_tagline_embeddings', \n",
    "        $top_k, \n",
    "        question_embedding\n",
    "        ) YIELD node AS movie, score\n",
    "    RETURN movie.title, movie.tagline, score\n",
    "    \"\"\",\n",
    "    params={\n",
    "        \"question_embedding\": question_embedding,\n",
    "        \"top_k\": 5\n",
    "    })\n",
    "\n",
    "prompt = f\"# Question:\\n{question}\\n\\n# Graph DB search results:\\n{result}\"\n",
    "\n",
    "system_msg = SystemMessage(\n",
    "            content=\"You will be given the user question along with the search result of that question over a Neo4j graph database. Give the user the proper answer.\"\n",
    "        )\n",
    "user_msg = HumanMessage(content=prompt)\n",
    "response = client.invoke([system_msg, user_msg])\n",
    "\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
